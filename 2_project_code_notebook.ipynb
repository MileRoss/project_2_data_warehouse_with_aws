{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# project_2_data_warehouse_with_redshift\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/Amazon_Web_Services_Logo.svg/2560px-Amazon_Web_Services_Logo.svg.png\" width=\"100\" height=\"100\">\n",
    "\n",
    "In this project, I'm a data engineer for a music streaming startup called Sparkify.\n",
    "\n",
    "Sparkify has expanded its user base and song database, and now wants to move its processes, data, and analytics applications to the cloud.\n",
    "\n",
    "Their data is stored in S3, with one directory containing JSON logs of user activity on the app, and another containing JSON metadata for the songs in their app.  \n",
    "My task is to build an ETL pipeline that extracts data from S3, stages it in Redshift, and transforms it into a set of dimensional tables.  \n",
    "Why? So their analytics team can continue finding insights into what songs users are listening to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import configparser\n",
    "import psycopg2\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#!pip install boto3\n",
    "import boto3\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup\n",
    "- Create a **new IAM user**.\n",
    "- Under **Attach existing policies directly**, assign **`AdministratorAccess`**.\n",
    "- Store the **access key** and **secret** in  **2_cloud_data_wh_redshift_boto3.cfg** file (same folder as this notebook).\n",
    "- Fill in the configuration file as follows:\n",
    "    ```\n",
    "    [AWS]\n",
    "    KEY=YOUR_AWS_KEY\n",
    "    SECRET=YOUR_AWS_SECRET\n",
    "    ```\n",
    "\n",
    "## 1.1. Troubleshoot  \n",
    "If your **keys are not working**, such as encountering an **InvalidAccessKeyId** error, follow these steps to **create a new pair of access keys**:\n",
    "1. Go to the **[IAM Dashboard](https://console.aws.amazon.com/iam/home)**.  \n",
    "2. View the details of the **Admin user** you created.  \n",
    "3. Select **Security Credentials** â†’ **Create access key**.  \n",
    "4. A new **Access Key ID** and **Secret** will be **generated**.  \n",
    "5. Update the `.cfg` file with the **new** credentials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Load configuration from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration file\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open(\"2.3_cloud_data_warehouse_aws_services_config_values.cfg\"))\n",
    "\n",
    "# AWS Credentials\n",
    "aws_key = config.get(\"AWS\", \"KEY\")\n",
    "aws_secret = config.get(\"AWS\", \"SECRET\")\n",
    "\n",
    "# AWS Region\n",
    "dwh_region = config.get(\"DWH\", \"DWH_REGION\")\n",
    "\n",
    "# IAM Role\n",
    "dwh_iam_role_name = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "# Redshift Cluster Configuration\n",
    "dwh_cluster_type = config.get(\"DWH\", \"DWH_CLUSTER_TYPE\")\n",
    "dwh_node_type = config.get(\"DWH\", \"DWH_NODE_TYPE\")\n",
    "dwh_num_nodes = config.get(\"DWH\", \"DWH_NUM_NODES\")\n",
    "dwh_cluster_identifier = config.get(\"DWH\", \"DWH_CLUSTER_IDENTIFIER\")\n",
    "\n",
    "# Database Configuration\n",
    "dwh_db = config.get(\"DWH\", \"DWH_DB\")\n",
    "dwh_db_user = config.get(\"DWH\", \"DWH_DB_USER\")\n",
    "dwh_db_password = config.get(\"DWH\", \"DWH_DB_PASSWORD\")\n",
    "dwh_port = config.get(\"DWH\", \"DWH_PORT\")\n",
    "\n",
    "# Display parameters in a DataFrame\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"Param\": [\n",
    "            \"DWH_REGION\",\n",
    "            \"DWH_IAM_ROLE_NAME\",\n",
    "            \"DWH_CLUSTER_TYPE\",\n",
    "            \"DWH_NODE_TYPE\",\n",
    "            \"DWH_NUM_NODES\",\n",
    "            \"DWH_CLUSTER_IDENTIFIER\",\n",
    "            \"DWH_DB\",\n",
    "            \"DWH_DB_USER\",\n",
    "            \"DWH_DB_PASSWORD\",\n",
    "            \"DWH_PORT\",\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            dwh_region,\n",
    "            dwh_iam_role_name,\n",
    "            dwh_cluster_type,\n",
    "            dwh_node_type,\n",
    "            dwh_num_nodes,\n",
    "            dwh_cluster_identifier,\n",
    "            dwh_db,\n",
    "            dwh_db_user,\n",
    "            dwh_db_password,\n",
    "            dwh_port,\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initialize AWS services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\n",
    "    \"iam\",\n",
    "    region_name=dwh_region,\n",
    "    aws_access_key_id=aws_key,\n",
    "    aws_secret_access_key=aws_secret,\n",
    ")\n",
    "\n",
    "ec2 = boto3.resource(\n",
    "    \"ec2\",\n",
    "    region_name=dwh_region,\n",
    "    aws_access_key_id=aws_key,\n",
    "    aws_secret_access_key=aws_secret,\n",
    ")\n",
    "\n",
    "s3 = boto3.resource(\n",
    "    \"s3\",\n",
    "    region_name=dwh_region,\n",
    "    aws_access_key_id=aws_key,\n",
    "    aws_secret_access_key=aws_secret,\n",
    ")\n",
    "\n",
    "redshift = boto3.client(\n",
    "    \"redshift\",\n",
    "    region_name=dwh_region,\n",
    "    aws_access_key_id=aws_key,\n",
    "    aws_secret_access_key=aws_secret,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. IAM Role Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the role\n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\")\n",
    "    dwh_role = iam.create_role(\n",
    "        Path=\"/\",\n",
    "        RoleName=dwh_iam_role_name,\n",
    "        Description=\"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps({\n",
    "            \"Statement\": [{\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"redshift.amazonaws.com\"}\n",
    "            }],\n",
    "            \"Version\": \"2012-10-17\"\n",
    "        })\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "# Attach Policy\n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=dwh_iam_role_name,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "# Get and print the IAM role ARN\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "role_arn = iam.get_role(RoleName=dwh_iam_role_name)[\"Role\"][\"Arn\"]\n",
    "\n",
    "print(role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. S3: check sample data, verify dataset presence.  \n",
    "The code lists and prints objects in the S3 bucket \"udacity-dend\" that have keys starting with \"song_data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_song_db_bucket = s3.Bucket(\"udacity-dend\")\n",
    "\n",
    "for obj in sample_song_db_bucket.objects.filter(Prefix=\"song_data\"):\n",
    "    print(obj)\n",
    "\n",
    "# Uncomment the following lines to list all objects in the bucket\n",
    "#for obj in sample_db_bucket.objects.all():\n",
    "#    print(obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Redshift: create cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(\n",
    "        # Parameters for hardware\n",
    "        ClusterType=dwh_cluster_type,\n",
    "        NodeType=dwh_node_type,\n",
    "        NumberOfNodes=int(dwh_num_nodes),\n",
    "\n",
    "        # Parameters for identifiers & credentials\n",
    "        DBName=dwh_db,\n",
    "        ClusterIdentifier=dwh_cluster_identifier,\n",
    "        MasterUsername=dwh_db_user,\n",
    "        MasterUserPassword=dwh_db_password,\n",
    "\n",
    "        # Parameter for role (to allow S3 access)\n",
    "        IamRoles=[role_arn],\n",
    "\n",
    "        # Make the cluster publicly accessible\n",
    "        PubliclyAccessible=True  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Monitor cluster status\n",
    "Format and display the cluster properties in a DataFrame. Run this block multiple times until **ClusterStatus** is **Available**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_redshift_props(props):\n",
    "    pd.set_option(\"display.max_colwidth\", None)\n",
    "    \n",
    "    keys_to_show = [\n",
    "        \"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \n",
    "        \"DBName\", \"Endpoint\", \"NumberOfNodes\", \"VpcId\", \"PubliclyAccessible\"\n",
    "    ]\n",
    "    \n",
    "    x = [(k, v) for k, v in props.items() if k in keys_to_show]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "my_cluster_props = redshift.describe_clusters(ClusterIdentifier=dwh_cluster_identifier)[\"Clusters\"][0]\n",
    "pretty_redshift_props(my_cluster_props)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Take note of endpoint & IAM role\n",
    "Do not run this unless **ClusterStatus** is **Available**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwh_endpoint = my_cluster_props[\"Endpoint\"][\"Address\"]\n",
    "dwh_role_arn = my_cluster_props[\"IamRoles\"][0][\"IamRoleArn\"]\n",
    "\n",
    "print(\"DWH_ENDPOINT:\", dwh_endpoint)\n",
    "print(\"DWH_ROLE_ARN:\", dwh_role_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Open network access\n",
    "## 3.1 Allow inbound traffic to Redshift by updating the security group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=my_cluster_props[\"VpcId\"])\n",
    "    default_sg = list(vpc.security_groups.all())[0]\n",
    "    print(default_sg)\n",
    "\n",
    "    default_sg.authorize_ingress(\n",
    "        GroupName=default_sg.group_name,\n",
    "        CidrIp=\"0.0.0.0/0\",\n",
    "        IpProtocol=\"TCP\",\n",
    "        FromPort=int(dwh_port),\n",
    "        ToPort=int(dwh_port),\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. PostgreSQL: connect to the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_string = f\"postgresql://{dwh_db_user}:{dwh_db_password}@{dwh_endpoint}:{dwh_port}/{dwh_db}?sslmode=verify-full&sslrootcert=system\"\n",
    "%sql $conn_string\n",
    "%sql SELECT current_user;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Queries\n",
    "\n",
    "Below is the content from sql_queries.py which will be imported into/by create_tables.py and etl.py queries. What it contains:\n",
    "- DROP [staging, fact, dimensions] tables if they exist\n",
    "- CREATE those tables \n",
    "- COPY staging tables FROM S3 to Redshift\n",
    "- INSERT data INTO fact and dimensions tables FROM the staging tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE SCHEMA IF NOT EXISTS song_play_analysis;\n",
    "SET search_path TO song_play_analysis;\n",
    "\n",
    "# DROP TABLES\n",
    "\n",
    "staging_events_table_drop = \"DROP TABLE IF EXISTS staging_events;\"\n",
    "staging_songs_table_drop = \"DROP TABLE IF EXISTS staging_songs;\"\n",
    "songplay_table_drop = \"DROP TABLE IF EXISTS songplays;\"\n",
    "user_table_drop = \"DROP TABLE IF EXISTS users;\"\n",
    "song_table_drop = \"DROP TABLE IF EXISTS songs;\"\n",
    "artist_table_drop = \"DROP TABLE IF EXISTS artists;\"\n",
    "time_table_drop = \"DROP TABLE IF EXISTS time;\"\n",
    "\n",
    "# CREATE TABLES\n",
    "\n",
    "staging_events_table_create= (\"\"\"\n",
    "CREATE TABLE staging_events (\n",
    "    artist varchar(max),\n",
    "    auth varchar(10),\n",
    "    firstName varchar(25),\n",
    "    gender varchar(1),\n",
    "    itemInSession integer sortkey distkey,\n",
    "    lastName varchar(25),\n",
    "    length float,\n",
    "    level varchar(4),\n",
    "    location varchar(max),\n",
    "    method varchar(4),\n",
    "    page varchar(max),\n",
    "    registration float,\n",
    "    sessionId integer,\n",
    "    song varchar(max),\n",
    "    status smallint,\n",
    "    ts bigint,\n",
    "    user_agent varchar(max),\n",
    "    user_id integer\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "staging_songs_table_create = (\"\"\"\n",
    "CREATE TABLE staging_songs (\n",
    "    num_songs integer sortkey distkey,\n",
    "    artist_id varchar(20),\n",
    "    artist_latitude float,\n",
    "    artist_longitude float,\n",
    "    artist_location varchar(max),\n",
    "    artist_name varchar(max),\n",
    "    song_id varchar(20),\n",
    "    title varchar(max),\n",
    "    duration float,\n",
    "    year smallint\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "songplay_table_create = (\"\"\"\n",
    "CREATE TABLE songplays (\n",
    "    songplay_id integer identity(0,1) not null sortkey distkey,\n",
    "    start_time timestamp not null,\n",
    "    user_id integer not null,\n",
    "    level varchar(4) not null,\n",
    "    song_id varchar(20) not null,\n",
    "    artist_id varchar(20) not null,\n",
    "    session_id integer not null,\n",
    "    location varchar(max),\n",
    "    user_agent varchar(max) not null\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "user_table_create = (\"\"\"\n",
    "CREATE TABLE users (\n",
    "    user_id integer not null sortkey distkey,\n",
    "    first_name varchar(25) not null,\n",
    "    last_name varchar(25) not null,\n",
    "    gender varchar(1) not null,\n",
    "    level varchar(4) not null\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "song_table_create = (\"\"\"\n",
    "CREATE TABLE songs (\n",
    "    song_id varchar(20) not null sortkey distkey,\n",
    "    title varchar(max) not null,\n",
    "    artist_id varchar(20) not null,\n",
    "    year smallint not null,\n",
    "    duration float not null\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "artist_table_create = (\"\"\"\n",
    "CREATE TABLE artists (\n",
    "    artist_id varchar(20) not null sortkey distkey,\n",
    "    name varchar(max),\n",
    "    location varchar(max),\n",
    "    latitude float,\n",
    "    longitude float\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "time_table_create = (\"\"\"\n",
    "CREATE TABLE time (\n",
    "    time_id integer identity(0,1) not null sortkey distkey,\n",
    "    start_time timestamp not null,\n",
    "    hour smallint not null,\n",
    "    day smallint not null,\n",
    "    week smallint not null,\n",
    "    month smallint not null,\n",
    "    year smallint not null,\n",
    "    weekday integer not null\n",
    ");\"\"\")\n",
    "\n",
    "# STAGING TABLES\n",
    "\n",
    "staging_events_copy = (\"\"\"\n",
    "COPY staging_events\n",
    "    FROM 's3://udacity-dend/log_data/'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    JSON 's3://udacity-dend/log_json_path.json'\n",
    "    COMPUPDATE off \n",
    "    REGION 'us-west-2'\n",
    "\"\"\".format(config.get('IAM_ROLE', 'ARN')))\n",
    "\n",
    "staging_songs_copy = (\"\"\"\n",
    "    COPY staging_songs \n",
    "    FROM 's3://udacity-dend/song_data/'\n",
    "    credentials 'aws_iam_role={}'\n",
    "    JSON 'auto'\n",
    "    COMPUPDATE off \n",
    "    REGION 'us-west-2'\n",
    "\"\"\".format(config.get('IAM_ROLE', 'ARN')))\n",
    "\n",
    "# FINAL TABLES\n",
    "\n",
    "songplay_table_insert = (\"\"\"\n",
    "INSERT INTO songplays (\n",
    "    start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    ")\n",
    "SELECT\n",
    "    TIMESTAMP 'epoch' + se.ts/1000 * INTERVAL '1 second' AS start_time,\n",
    "    se.user_id,\n",
    "    se.level,\n",
    "    ss.song_id,\n",
    "    ss.artist_id,\n",
    "    se.sessionId,\n",
    "    se.location,\n",
    "    se.user_agent\n",
    "FROM staging_events se\n",
    "JOIN staging_songs ss\n",
    "  ON se.song = ss.title \n",
    "  AND se.artist = ss.artist_name\n",
    "  AND se.length = ss.durationWHERE se.page = 'NextSong';\n",
    "\"\"\")\n",
    "\n",
    "user_table_insert = (\"\"\"\n",
    "INSERT INTO users (\n",
    "    user_id, first_name, last_name, gender, level\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    user_id,\n",
    "    firstName,\n",
    "    lastName,\n",
    "    gender,\n",
    "    level\n",
    "FROM staging_events\n",
    "WHERE page = 'NextSong' AND user_id IS NOT NULL;\n",
    "\"\"\")\n",
    "\n",
    "song_table_insert = (\"\"\"\n",
    "INSERT INTO songs (\n",
    "    song_id, title, artist_id, year, duration\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    song_id,\n",
    "    title,\n",
    "    artist_id,\n",
    "    year,\n",
    "    duration\n",
    "FROM staging_songs;\n",
    "\"\"\")\n",
    "\n",
    "artist_table_insert = (\"\"\"\n",
    "INSERT INTO artists (\n",
    "    artist_id, name, location, latitude, longitude\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    artist_id,\n",
    "    artist_name AS name,\n",
    "    artist_location AS location,\n",
    "    artist_latitude AS latitude,\n",
    "    artist_longitude AS longitude\n",
    "FROM staging_songs;\n",
    "\"\"\")\n",
    "\n",
    "time_table_insert = (\"\"\"\n",
    "INSERT INTO time (\n",
    "    start_time, hour, day, week, month, year, weekday\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    start_time,\n",
    "    EXTRACT(hour FROM start_time),\n",
    "    EXTRACT(day FROM start_time),\n",
    "    EXTRACT(week FROM start_time),\n",
    "    EXTRACT(month FROM start_time),\n",
    "    EXTRACT(year FROM start_time),\n",
    "    EXTRACT(weekday FROM start_time)\n",
    "FROM (\n",
    "    SELECT TIMESTAMP 'epoch' + ts/1000 * INTERVAL '1 second' AS start_time\n",
    "    FROM staging_events\n",
    "    WHERE page = 'NextSong'\n",
    ") AS time_data;\n",
    "\"\"\")\n",
    "\n",
    "# ANALYTICS QUERIES\n",
    "\n",
    "top_artists_query = (\"\"\"\n",
    "SELECT a.name, COUNT(sp.artist_id) AS artist_count\n",
    "FROM artists a\n",
    "JOIN songplays sp ON a.artist_id = sp.artist_id\n",
    "GROUP BY a.name\n",
    "ORDER BY artist_count DESC\n",
    "LIMIT 5;\n",
    "\"\"\")\n",
    "\n",
    "top_genders_query = (\"\"\"\n",
    "SELECT gender, COUNT(gender) AS gender_count\n",
    "FROM staging_events\n",
    "WHERE page = 'NextSong'\n",
    "GROUP BY gender\n",
    "ORDER BY gender_count DESC;\n",
    "\"\"\")\n",
    "\n",
    "peak_hours_query = (\"\"\"\n",
    "SELECT hour, COUNT(hour) AS peak_hour_count\n",
    "FROM time\n",
    "GROUP BY hour\n",
    "ORDER BY peak_hour_count DESC\n",
    "LIMIT 5;\n",
    "\"\"\")\n",
    "\n",
    "# QUERY LISTS\n",
    "\n",
    "create_table_queries = [staging_events_table_create, staging_songs_table_create, songplay_table_create, user_table_create, song_table_create, artist_table_create, time_table_create]\n",
    "drop_table_queries = [staging_events_table_drop, staging_songs_table_drop, songplay_table_drop, user_table_drop, song_table_drop, artist_table_drop, time_table_drop]\n",
    "copy_table_queries = [staging_events_copy, staging_songs_copy]\n",
    "insert_table_queries = [songplay_table_insert, user_table_insert, song_table_insert, artist_table_insert, time_table_insert]\n",
    "\n",
    "analytics_queries = [\n",
    "    (\"Who are the top 5 most played artists?\", top_artists_query),\n",
    "    (\"Which gender listened to more songs?\", top_genders_query),\n",
    "    (\"What are the 5 peak listening hours?\", peak_hours_query)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create tables\n",
    "\n",
    "Below is the content from create_tables.py script.  \n",
    "The script connects to the database, creates the tables, and drops them beforehand if they already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_tables(cur, conn):\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Check for data loading errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT *\n",
    "FROM stl_load_errors\n",
    "ORDER BY starttime DESC\n",
    "LIMIT 5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Check for created and loaded tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql SELECT tablename FROM pg_tables WHERE schemaname = 'song_play_analysis';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Check for the tables' characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SET search_path TO song_play_analysis;\n",
    "\n",
    "SELECT table_name, column_name, data_type, is_nullable, column_default\n",
    "FROM information_schema.columns\n",
    "WHERE table_schema = 'song_play_analysis'\n",
    "ORDER BY table_name, column_name;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. ETL\n",
    "\n",
    "Below is the content from the etl.py. What this script does:\n",
    "- connects to the Sparkify Redshift database, \n",
    "- loads log_data and song_data into staging tables, \n",
    "- transforms them into the five tables, \n",
    "- runs three analytics queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_staging_tables(cur, conn):\n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def insert_tables(cur, conn):\n",
    "    for query in insert_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def run_analytics_queries(cur, conn):\n",
    "    print(\"\\nâ”€â”€â”€ Songplay Analytics â”€â”€â”€\")\n",
    "    for question, query in analytics_queries:\n",
    "        print(f\"\\n{question}\")\n",
    "        cur.execute(query)\n",
    "        rows = cur.fetchall()\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "    conn.commit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    load_staging_tables(cur, conn)\n",
    "    insert_tables(cur, conn)\n",
    "    run_analytics_queries(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Clean up AWS resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1. Delete cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift.delete_cluster( ClusterIdentifier=dwh_cluster_identifier, SkipFinalClusterSnapshot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block several times until the cluster is deleted = `ClusterNotFoundFault`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=dwh_cluster_identifier)['Clusters'][0]\n",
    "pretty_redshift_props(myClusterProps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2. Delete IAM role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam.detach_role_policy(RoleName=dwh_iam_role_name, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "iam.delete_role(RoleName=dwh_iam_role_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
